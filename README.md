# Nâ€‘Armed Bandit Simulator ðŸŽ°

This repository implements a classical **Nâ€‘Armed Bandit** problem using the **Îµâ€‘greedy strategy** (and optionally other algorithms) in Python via a Jupyter Notebook. The agent learns to maximize long-term rewards by balancing **exploration** and **exploitation** over multiple independent bandit "arms" with unknown payoff distributions.

---

## ðŸ“Œ Features

- Configurable number of arms (`n_arms`) and trials (`n_steps`)
- Implemented **Îµâ€‘greedy algorithm** with tunable Îµ
- Collects and visualizes:
  - Cumulative reward over time
  - Optimal action percentage per time step
  - Optional comparison between greedy and exploratory policies
- Supports multiple independent runs to evaluate statistical performance
- Simple, educational implementation ideal for learning reinforcement learning basics

---



## ðŸ“– Overview of the Notebook

- **Initialization**: Defines true payoff values (`q*(a)`) via sampled normal distributions.
- **Îµâ€‘greedy strategy**:
  - With probability Îµ, choose a random arm (explore)
  - Otherwise, choose the currently estimated best arm (exploit)
- **Simulation loop**:
  - For each time step:
    - Choose an arm
    - Receive reward (sampled from that armâ€™s distribution)
    - Update estimated averages \(Q(a)\)
  - Tracks running reward and optimal action statistics
- **Visualization**:
  - Plots average reward vs. time
  - (Optional) plots % of optimal actions vs. time

---

## ðŸ“Š What Youâ€™ll Learn

- Core concepts of the multiâ€‘armed bandit problem
- Trade-off between exploration (trying new options) vs. exploitation (using known good options)
- How Îµ impacts learning performance over time
- Techniques for aggregating results across multiple simulation runs

---

## ðŸ§© Extensions & Experimentation

Consider expanding the project by:

- Varying Îµ (e.g., 0, 0.01, 0.1, 0.2) and comparing results
- Implementing other strategies like **softmax**, **UCB**, or **Thompson Sampling**
- Adding non-stationarity (i.e., changing arm reward distributions over time)
- Packaging simulation results for quantitative comparison

---

Developed by **ShivaInja21**
